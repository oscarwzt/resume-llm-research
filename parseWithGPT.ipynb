{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "import shutil\n",
    "import time\n",
    "import requests\n",
    "from utils import * \n",
    "from requests.exceptions import HTTPError, RequestException\n",
    "with open(\"chatGPT_API_KEY.txt\") as f:\n",
    "    API_KEY = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = API_KEY\n",
    "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "\n",
    "def getResume(res_num = None, name = None, path = \"resume_corpus\"):\n",
    "    if res_num is None:\n",
    "        res_num = random.randint(0, len(os.listdir(path)) - 1)\n",
    "        \n",
    "    resume = name if name else os.listdir(path)[res_num]\n",
    "    file_path = os.path.join(path, resume)\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8', errors=\"replace\") as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def getNumTokens(text, tokenizer = tokenizer):\n",
    "    return len(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"resumes_groupby_tokens/resumes_400_600.txt\") as f:\n",
    "    resumes = f.read().split(\"\\n\")\n",
    "    \n",
    "message_list = [[{'role': \"system\", \"content\": SYSINS},\n",
    "                    {\"role\": \"user\", \"content\": INSTRUCTION + getCleanResume(res)}]\n",
    "                for res in resumes[:10]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please parse the resume into the following JSON format: \n",
      "{\n",
      "    \"Basic_Info\": {\n",
      "        \"Current_Title\": \"\",\n",
      "        \"Current_Company\": \"\",\n",
      "        \"Location\": \"\",\n",
      "        \"Bio\": \"\"\n",
      "    },\n",
      "    \"Experience\": [\n",
      "        {\n",
      "            \"Job_Title\": \"\",\n",
      "            \"Company\": \"\",\n",
      "            \"Location\": \"\",\n",
      "            \"Start_Date\": \"\",\n",
      "            \"End_Date\": \"\",\n",
      "            \"Responsibilities\": \"\"\n",
      "        }\n",
      "    ],\n",
      "    \"Education\": [\n",
      "        {\n",
      "            \"Degree\": \"\",\n",
      "            \"Field\": \"\",\n",
      "            \"Institution\": \"\",\n",
      "            \"Location\": \"\",\n",
      "            \"Graduation_Date\": \"\"\n",
      "        }\n",
      "    ],\n",
      "    \"Projects\": [\n",
      "        {\n",
      "            \"Project_Title\": \"\",\n",
      "            \"Description\": \"\",\n",
      "            \"Date\": \"\"\n",
      "        }\n",
      "    ],\n",
      "    \"Skills\": [],\n",
      "    \"Technical_Skills\": [],\n",
      "    \"Links\": [],\n",
      "    \"Certifications\": [\n",
      "        {\n",
      "            \"Certification_Title\": \"\",\n",
      "            \"Issuing_Organization\": \"\",\n",
      "            \"Date_Issued\": \"\"\n",
      "        }\n",
      "    ],\n",
      "    \"Awards\": [\n",
      "        {\n",
      "            \"Award_Title\": \"\",\n",
      "            \"Issuing_Organization\": \"\"\n",
      "        }\n",
      "    ],\n",
      "    \"Publications\": [\n",
      "        {\n",
      "            \"Publication_Title\": \"\",\n",
      "            \"Date\": \"\"\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "Project Manager/Linux System and Database Admin for Voice \n",
      "Project Manager /Linux System and Database Admin for Voice Project Manager/Linux System and Database Admin for Voice - Market Price Information Work Experience Project Manager/Linux System and Database Admin for Voice Market Price Information Present System Project at United Purpose (International NGO) formerly known as Concern Universal. United Nations November 2017 to December 2017 consultancy contract to offer service for International Trade- Centre: I was contracted to conduct an ICT countrywide survey and also contribute in the analysis of the findings, focusing on the ICT component of the Youth Empowerment Project YEP in The Gambia. The aim of the mission was for me to work with the international IT expert, to identify the entrepreneurship and job creation- opportunities for the Youths and offer a strategic blueprint for further action by YEP in the area of ICT in The- Gambia. Technical Project Manager Concern Universal (now United Purpose), International NGO October 2014 to December 2015 for Better Tech for CSOs & MSMEs Project, Sponsored by US Embassy.- Project aims to empower beneficiaries with Internet, Software Solutions, Basic IT Training and Online presence. General IT Support Technician for cooperate and home Clients Netpage Co Ltd October 2009 to February 2014 Windows Server 2008 Installation and Support for both Server and Clients Machines- Website updates for clients Education School Certificate St. Peter's Technical Senior Secondary June 2005 Skills Database, Mysql, Linux, C++, Html, Php, Android, Java, Linux Administrator Links http://www.thewebway.info http://gamcybersecurityalliance.com Additional Information SKILLS/PROGRAMMING LANGUAGES- HTML(V-Good), PHP(V-Good), MySQL(V-Good), Java(V-Good), C(Average), C++(Average),- Android(Good) and Linux(Good).- My Website link to my Android and Database Web Applications: http://thewebway.info/ projects.html\n"
     ]
    }
   ],
   "source": [
    "print(message_list[0][1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_35_16K = \"gpt-3.5-turbo-16k\"\n",
    "GPT_35 = \"gpt-3.5-turbo\"\n",
    "GPT_4 = \"gpt-4\"\n",
    "\n",
    "            \n",
    "output_format_skeleton = \"\"\"\n",
    "{\n",
    "    \"Basic_Info\": {\n",
    "        \"Current_Title\": \"\",\n",
    "        \"Current_Company\": \"\",\n",
    "        \"Location\": \"\",\n",
    "        \"Bio\": \"\"\n",
    "    },\n",
    "    \"Experience\": [\n",
    "        {\n",
    "            \"Job_Title\": \"\",\n",
    "            \"Company\": \"\",\n",
    "            \"Location\": \"\",\n",
    "            \"Start_Date\": \"\",\n",
    "            \"End_Date\": \"\",\n",
    "            \"Responsibilities\": \"\"\n",
    "        }\n",
    "    ],\n",
    "    \"Education\": [\n",
    "        {\n",
    "            \"Degree\": \"\",\n",
    "            \"Field\": \"\",\n",
    "            \"Institution\": \"\",\n",
    "            \"Location\": \"\",\n",
    "            \"Graduation_Date\": \"\"\n",
    "        }\n",
    "    ],\n",
    "    \"Projects\": [\n",
    "        {\n",
    "            \"Project_Title\": \"\",\n",
    "            \"Description\": \"\",\n",
    "            \"Date\": \"\"\n",
    "        }\n",
    "    ],\n",
    "    \"Skills\": [],\n",
    "    \"Technical_Skills\": [],\n",
    "    \"Links\": [],\n",
    "    \"Certifications\": [\n",
    "        {\n",
    "            \"Certification_Title\": \"\",\n",
    "            \"Issuing_Organization\": \"\",\n",
    "            \"Date_Issued\": \"\"\n",
    "        }\n",
    "    ],\n",
    "    \"Awards\": [\n",
    "        {\n",
    "            \"Award_Title\": \"\",\n",
    "            \"Issuing_Organization\": \"\"\n",
    "        }\n",
    "    ],\n",
    "    \"Publications\": [\n",
    "        {\n",
    "            \"Publication_Title\": \"\",\n",
    "            \"Date\": \"\"\n",
    "        }\n",
    "    ]\n",
    "}\"\"\"\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"resume_corpus\"\n",
    "#task = \"turn the following resumes into JSON format with their file names as their keys not including the file extension. Parse the ENTIRE resume, and do not omit any part due to length! Do not add any extra sections!\"\n",
    "instruction = f\"Please parse the resume into the following JSON format: \\n{output_format_skeleton}\"\n",
    "resume = getResume(name = \"03061.txt\")\n",
    "#print(resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please parse the resume into the following JSON format: \n",
      "\n",
      "{\n",
      "    \"Basic_Info\": {\n",
      "        \"Current_Title\": \"\",\n",
      "        \"Current_Company\": \"\",\n",
      "        \"Location\": \"\",\n",
      "        \"Bio\": \"\"\n",
      "    },\n",
      "    \"Experience\": [\n",
      "        {\n",
      "            \"Job_Title\": \"\",\n",
      "            \"Company\": \"\",\n",
      "            \"Location\": \"\",\n",
      "            \"Start_Date\": \"\",\n",
      "            \"End_Date\": \"\",\n",
      "            \"Responsibilities\": \"\"\n",
      "        }\n",
      "    ],\n",
      "    \"Education\": [\n",
      "        {\n",
      "            \"Degree\": \"\",\n",
      "            \"Field\": \"\",\n",
      "            \"Institution\": \"\",\n",
      "            \"Location\": \"\",\n",
      "            \"Graduation_Date\": \"\"\n",
      "        }\n",
      "    ],\n",
      "    \"Projects\": [\n",
      "        {\n",
      "            \"Project_Title\": \"\",\n",
      "            \"Description\": \"\",\n",
      "            \"Date\": \"\"\n",
      "        }\n",
      "    ],\n",
      "    \"Skills\": [],\n",
      "    \"Technical_Skills\": [],\n",
      "    \"Links\": [],\n",
      "    \"Certifications\": [\n",
      "        {\n",
      "            \"Certification_Title\": \"\",\n",
      "            \"Issuing_Organization\": \"\",\n",
      "            \"Date_Issued\": \"\"\n",
      "        }\n",
      "    ],\n",
      "    \"Awards\": [\n",
      "        {\n",
      "            \"Award_Title\": \"\",\n",
      "            \"Issuing_Organization\": \"\"\n",
      "        }\n",
      "    ],\n",
      "    \"Publications\": [\n",
      "        {\n",
      "            \"Publication_Title\": \"\",\n",
      "            \"Date\": \"\"\n",
      "        }\n",
      "    ]\n",
      "}Application DBA Application DBA Application DBA - Northern Trust Chicago, IL I am a DBA with 6 years of hands-on experience. My body of work includes Installation, Configuration, Patch administration, Migration, Upgrades, Backup and Recovery, Cloning, Replication, Database Security, Memory Management, Data Modeling, Performance Monitoring and Tuning, RAC, Data Guard, Grid Control, Oracle Golden gate. I have worked on Oracle 12c, 11g, 10g versions running on UNIX, LINUX and Windows platforms.� � Programming experience as an Oracle PL/SQL Developer in Analysis, Design and Implementation of Business Applications using the Oracle Relational Database Management System (RDBMS).� Involved in all phases of the SDLC (Software Development Life Cycle) from analysis, design, development, testing, implementation and maintenance with timely delivery against aggressive deadlines.� Expertise in Client-Server application development using Oracle 11g/10g/9i/8i, PL/SQL, SQL *PLUS, TOAD and SQL*LOADER.� Strong proficiency in supporting Production Cloud environments (AWS, VMWare) as well as traditional managed hosted environments.� Effectively made use of Table Functions, Indexes, Table Partitioning, Collections, Analytical functions, Materialized Views, Query Re-Write and Transportable table spaces.� Strong experience in Data warehouse concepts, ETL.� Good knowledge on logical and physical Data Modeling using normalizing Techniques.� Created Tables, Views, Constraints, Index (B Tree, Bitmap and Function Based).� Developed Complex database objects like Stored Procedures, Functions, Packages and Triggers using SQL and PL/SQL.� Developed materialized views for data replication in distributed environments.� Excellent technical and analytical skills with clear understanding of design goals of ER modeling for OLTP and dimension modeling for OLAP.� Experience in Oracle supplied packages, Dynamic SQL, Records and PL/SQL Tables.� Loaded Data into Oracle Tables using SQL Loader.� Partitioned large Tables using range partition technique.� Experience with Oracle Supplied Packages such as DBMS_SQL, DBMS_JOB and UTL_FILE.� Created Packages and Procedures to automatically drop table indexes and create indexes for the tables.� Worked extensively on Ref Cursor, External Tables and Collections.� Expertise in Dynamic SQL, Collections and Exception handling.� Experience in SQL performance tuning using Cost-Based Optimization (CBO).� Good knowledge of key Oracle performance related features such as Query Optimizer, Execution Plans and Indexes.� Experience with Performance Tuning for Oracle RDBMS using Explain Plan and HINTS.� Experience in system Monitoring and Database Performance Tuning using Explain plan, SQL Trace, ASH, TKPROF and AWR� Provided valuable inputs for various performance tuning issues.� Expertise in Performing Tuning Database - Tuning involved I/O, Memory, CPU utilization, SQL Tuning, SGA tuning, Shared Pool tuning, Buffers and Disk I/O tuning using Oracle's regular performance tuning tools like Explain Plan, STATSPACK (perfstat), SQL*Trace, TKPROF, Explain plan, AWR and ADDM reports, Advisors as SQL Tuning, SQL Advice, Undo, Segment, MTTR, Memory.� Experience in ETL techniques and Analysis and Reporting including hands on experience with the Reporting tools such as Cogno.� Created Shell Scripts for invoking SQL scripts and scheduled them using crontab.� Excellent knowledge about managing large-scale OLAP and OLTP databases 5+TB size in HA and DR setup.� Monitoring the ASM instances on the multiple nodes and adding the disk to the disk group when required.� Monitoring the Database health check and growth with scripts.� Expertise concerning migration/consolidation of databases using Oracle 12c multitenant.� Planned disaster recovery scenarios with different recovery procedures and methods like Data Guard & Standby Database.� Executed and implemented user helpdesk tickets of various issues via REMEDY database from creating user accounts, read-only accounts on a database & table down to assisting them with logging into the database. Experienced in using 10g features like 10g RMAN, Data pump Flash-Back Recovery and ASM.� Knowledge of Database Recovery Techniques and implementation of those recovery techniques to ensure business continuity.� Expertise in maintaining Database Security using auditing.� Experience in Data Migrations.� Highly experienced in performance monitoring/tuning/troubleshooting.� Oracle Database performance-tuning services with EXPLAIN PLAN, TKPROF, STATSPACK, SQL TRACE. Applying patches under UNIX platform.� Planned and implemented high availability solutions such as Real Application Cluster (RAC) in Oracle 11gR2 Grid and 10g on ASM and OCSF2 file systems.� Highly experienced in implementing Oracle's Transportable Tablespace (TTS) feature using Data pump Export/Import (Oracle 11g, 10g) allowing users to quickly move a user Tablespace across Oracle databases.� Expertise in implementing data refreshes (at database, schema & table level) using RMAN utility and Datapump, conventional Export/Import of Oracle 11g, 10g.� Extensively implemented Datapump, conventional Export/Import (Oracle 11g, 10g) for migration and logical backup of database.� Tablespace & Data file creation and Maintenance by Adding and Resizing data file if needed� Keeping the databases up to date with respect to Latest PSU's and very comfortable in applying patches.� Responsible for database maintenance, backups, recovery, patching, upgrades and performance tuning, user access control.� UNIX Systems Administration - Database and application integration efforts. Heavy UNIX shell scripting, CRON jobs.� Starting/Shutting down the RAC databases and Creating and Relocating RAC database services to instances.� Switchover and Failover RAC database with Data guard Broker.� Implementation of RAC using ASM on 3 Nodes.� Monitoring of cluster ware files i.e., OCR files & Voting Disk files.� Designed and implemented Oracle Real Application Clusters (RAC) on Linux.� Experience in installation and configuration of Oracle cluster ware and Automatic Storage Management (ASM) in RAC environment.� Experience in conversion of Single Node Instance to multimode RAC with ASM, and adding instance to existing RAC.� Experience in Migration of Oracle Databases from non-RAC to RAC and non-ASM to ASM Instances.� Experience in Real Application Clusters (RAC) setup with 4 nodes, configuration and cluster interconnection with Automatic Storage Management (ASM). Authorized to work in the US for any employer Work Experience Application DBA Northern Trust - Chicago, IL March 2019 to Present Responsible for database maintenance, backups, recovery, performance tuning.� Import and export of the database objects to move data between 10g, 11g and 12C RAC environments. Expert on change management and service management ( service request , change request , IDM request etc..).� Experience in system Monitoring and Database Performance Tuning using Explain plan, SQL Trace, TKPROF, AWR ASH, SQL tuning advisor and also Cluster Interconnect Tuning (RAC).� Provided valuable inputs for various performance tuning issues.� Created new database sequence after database refresh.� Good experience on Capacity planning, Architecture planning; Along with database lock, blocking session, and space management.� Responsible for setting and managing USER, Granting required privileges and revoking them as well when needed.� Creation or modification of custom tables, views, sequences, procedures, functions, indexes, tablespaces and other DDL requests.� Use tools as SQL developer and custom scripts; while creating users and maintaining password rights.� build positive and professional business relationships with internal and external clients; work independently and handle a team, work with business to gather requirements. Oracle Database Administrator Cotiviti - Atlanta, GA January 2016 to February 2019 Supporting multiple databases for production, development, test and staging purposes on Sun Solaris and Windows environments.� Created and monitored Physical standby databases on maximum protection, performance mode using Data guard for Disaster Recovery and High Availability.� Upgrading Grid Infrastructure and databases from 11.2.0.3 to 11.2.0.4.� Upgrading Grid Infrastructure and databases from 11.2.0.4 to 12c.� Configured and implemented Oracle 11g ACTIVE DATAGAURD for Database Disaster Recovery by using DATA GAURD broker (DGMDRL)� Installed and configured 4 node server side load balancing Oracle RAC 11g, 12c databases on Linux with ASM and Data Guard.� Production support database on 11g and 12c versions with standalone and RAC environment.� Set up user profiles and roles as per the access level of user for the data and application security.� Re-wrote the shell scripts to monitor Golden Gate processes various states (like STOPPED/ABENDED/LAG/HUNG) to reduce number of alerts by grouping them together by environment.� Installation and configuration of Oracle Golden gate.� Responsible for implementation and administration of disaster recovery strategies for 24/7 operations and deployment of high availability systems on Oracle 11g, using Oracle RAC, Data-Guard, Streams, ASM, RMAN.� Configured deployed and administered 4 node RAC clusters on Solaris 10 and Redhat Enterprise Linux 6.0, using ASM and Raw Devices on Oracle 11gR2.� Table created with Daily Range partition, indexes and script automated which add/remove partition as per client requirement.� Monitoring and optimizing the performance of the database/application using OEM along with homespun scripts.� Capacity planning: Allocate system storage and plan future storage requirements.� Oracle HA Stack: Install, Configure, Setup, Maintain and Support Oracle 11gR1, 11gR2, 12c RAC, Physical and Logical Standby database for Test, Development and Production for various home-developed applications like Portfolio Management Systems and vendor applications like SAP Business solutions� Involve in 24x7 on-call support for 12 databases of 2.0 TB 4 Node production 10g and 11g RAC databases using Oracle 12c Cloud Control for Alert Monitoring and Notification� Upgrade DBs from 11.2.0.4 to 12.1. Responsible for setting and managing USER MANAGEMENT, SPACE MANAGEMENT Granting required privileges to users.� Applying upgrade patch, maintenance and interim (opatch) patches on all the databases.� Refreshing Dev and Test instances with data from Production on a regular basis.� Developed PL/SQL packages, DML, DDL, Oracle tables, Stored Procedures, functions, cursors, triggers and UNIX shell scripts.� Thorough knowledge in setting up infrastructure for databases in huge datacenters.� Installed Oracle 11g on Linux platform using ASM.� RMAN point in time recovery and Instance recovery done on 11g.� Developed compliance reports and performed auditing at higher levels.� Experience in system Monitoring and Database Performance Tuning using Explain plan, SQL Trace, TKPROF, AWR and also Cluster Interconnect Tuning (RAC).� Provided valuable inputs for various performance tuning issues.� Proficient in performance tuning using Explain Plan, STATSPACK, TKPROF, AWR, ADDM. Performance Tuning of the database (Memory Tuning, I/O Tuning)� Expertise in Performing Tuning Database - Tuning involved I/O, Memory, CPU utilization, SQL Tuning, SGA tuning, Shared Pool tuning, Buffers and Disk I/O tuning using Oracle's regular performance tuning tools like Explain Plan, STATSPACK, SQL*Trace, AWR, ADDM, ASH, SQL tuning advisor etc. Done resetting of storage parameters by series of exports and imports of tables, used conventional as well as direct path loads to upload the data into the server using SQL loader.� Scheduling repetitive activities like gathering schema statistics, exports, table space usage report using crontab.� Upgrade and Migration of Database from 11.2.0.2to 12.2.0.2 and applying patches whenever required.� Efficiently performed installation, setup and creation of four node cluster RAC with Oracle11.2.0.3 database using GRID infrastructure with ASM file systems on RHEL 5.5.� Maintenance of Oracle 11gR2 Real Application Cluster (RAC) Database for High Availability, Scalability and Performance by determining the best Cluster architecture, choosing the best hardware configuration for Oracle RAC� Troubleshooting of various problems on AWS Production databases.� Successful in installation/maintenance of Physical Standby database using Oracle Data Guard for Oracle 11.2.0.3 and Oracle 12.1.0 RAC databases� Performing monthly and annual performance reports for trend analysis and capacity planning� Assist in SQL tuning and providing consultation such as creating Materialized views, adding indices, dropping unnecessary indices, using hints wherever possible, amongst others.� Worked on both VMS and UNIX environments.� � Troubleshooting of various database performances by proper diagnosis at all levels like SQL, PL/SQL, database design, database tables, indexes, Instance, memory, operating system and java calls.� Knowledge in Oracle Data warehouse DBA skills relating to administration and management of VLDB environments, Oracle OLTP.� Experience in writing UNIX/SHELL scripts for providing the reports� Installed and configured Oracle 11g database on a test server using Oracle standard procedures and OFA, for performance testing and future 10g production implementation. Oracle Database Administrator News Bank, Inc - Chester, VT July 2013 to December 2015 Performed monthly and annual performance reports for trend analysis and capacity planning.� Provided Development DBA support for developer queries and requests, which includes query tuning, server tuning, identifying problematic code, etc� Suggested and improved system design and architecture, this includes built New Oracle 10g/11g RAC Databases, ASM Physical and standby systems, enabling LDAP authentication and configuring RMAN backups.� Monitored the operating system response in terms of CPU usage, disk space, and swap space by using various UNIX utilities like SAR, VMSTAT, IOSTAT and TOP.� Used import/export utilities for cloning/migration of small sized databases and Data pump import/export to move data between 10g and 10g/11g environments.� Implemented proactive monitoring using Tuning Pack, Diagnostics Pack, and STATSPACK, SQL Trace & TKPROF, EXPLAIN PLAN.� Database monitoring/maintenance, replication process and performance tuning are done using OEM (Oracle Enterprise Manager).� Trouble shooting and performance tuning golden gate replications, monitoring scripts for Golden gate in Unix scripting� Install and configure Cloud Control 12c, deploy 12c agents, plug-in installations on the target servers and monitor databases.� Configure Oracle Advanced Security on 12c databases to implement Transparent Data Encryption at tablespace, table and column levels. Also configured Redaction using OEM's Policy expression builder.� Implemented Flashback technology of the Oracle 11g, 10g along with Recycle bin for a faster recovery of the databases and database objects.� Performed full & incremental backup (cumulative and differential) using RMAN and implemented recovery strategies.� Refreshed/cloned databases using RMAN utility.� Performed Database upgrade from Oracle 10g (10.2.0.2) to Oracle 10g (10.2.0.5) and to Oracle 11g (11.1.0.7) RAC database.� Implemented and configured 10g grid control on Production server and 11g grid control on Responded to all database alerts, warnings, errors in a timely, consistent manner.� Escalated complex issues to internal team members and third-party group members as appropriate.� Extensively used AWR, ADDM and explain plan for periodic performance tuning.� Implementing Data pump, conventional export/import utility of Oracle 11g, 10g for re-organizing Databases/Schemas/Tables to improve the performance.� Used import/export utilities for cloning/migration of small sized databases and Data pump import/export to move data between 10g and 11g environments.� Implemented proactive monitoring using Tuning Pack, Diagnostics Pack, and STATSPACK, SQL Trace & TKPROF, EXPLAIN PLAN.� Database monitoring/maintenance, replication process and performance tuning are done using OEM (Oracle Enterprise Manager).� Automated the processes like moving files, managing alert log, efficient backup by developing UNIX scripts.� Involved in the installation, configuration and extended support to Oracle 11g, 10g two Node RAC (Real Application Cluster) with ASM file system on Sun Solaris platform.� Involved in converting a single instance database to Oracle 11g, 10g RAC databases.� Utilized TOAD for database management.� � TOOLS USED:� Tuning Tools� TKPROF, EXPLAIN PLAN, STATSPACK, AWR, and ADDM� � Tools and Utilities� RMAN, OEM, SQL*Loader, EXP, IMP, Data pump, TOAD, Oracle SQL Developer� � RDBMS� Oracle 10g, 11g, 12c� � Operating Systems� HP-UX 10.x, 11.x. Sun RHEL 2.1, 3.x, 4.x, 5.x. Linux Enterprise Education Bachelor's Skills LINUX, SUN, UX, HP-UX\n"
     ]
    }
   ],
   "source": [
    "print(instruction + resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "[20, 21, 22, 23, 24, 25]\n"
     ]
    }
   ],
   "source": [
    "a = list(range(26))\n",
    "for i in range(0, 26, 10):\n",
    "    print(a[i:i+10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"parsed_resume.json\") as f:\n",
    "#     parsed_resume = json.load(f)\n",
    "\n",
    "# avai_res = [x for x in os.listdir(\"resume_corpus\") if x not in parsed_resume.keys()]\n",
    "# res_to_use = random.sample(avai_res, 10)\n",
    "res_to_use = list(temp.keys())\n",
    "sample_parse_res = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for res in tqdm(res_to_use):\n",
    "    resume = getResume(name = res)\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model = GPT_35_16K,\n",
    "        messages = [\n",
    "            {'role': \"system\", \"content\": \"You are a detail oriented resume parser who does not miss any information.\"},\n",
    "            {\"role\": \"user\", \"content\": instruction + resume}\n",
    "            ],\n",
    "        temperature = 0,\n",
    "        presence_penalty = -0.8\n",
    "    )\n",
    "    \n",
    "    sample_parse_res[res] = eval(response['choices'][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sample_Chatgpt_parse_res.json\", \"w\") as f:\n",
    "    json.dump(sample_parse_res, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['21770.txt', '08995.txt', '03249.txt', '03114.txt', '16860.txt', '02107.txt', '06375.txt', '01690.txt', '19202.txt', '25012.txt'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"llama_parsed_resume.json\") as f:\n",
    "    temp = json.load(f)\n",
    "temp = {k: v for k, v in temp.items() if k in sample_parse_res.keys()}\n",
    "with open(\"sample_Llama2_parse_res.json\", \"w\") as f:\n",
    "    json.dump(temp, f, indent=4)\n",
    "# with open(\"sample_parse_res.json\", \"w\") as f:\n",
    "#     json.dump(sample_parse_res, f, indent=4)\n",
    "temp.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt35_response_dict = {}\n",
    "gpt35_resume_dict = {}\n",
    "jj = 0\n",
    "path = \"resume_corpus\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'resume_03061', 'resume_25918', 'resume_18327', 'resume_00568',\n",
    "\n",
    "def joinResumes(resume_list, path = path):\n",
    "    joint_resume = \"\"\n",
    "    \n",
    "    for resume in resume_list:\n",
    "        with open(os.path.join(path, resume), 'r', encoding='utf-8', errors=\"replace\") as f:\n",
    "            text = f.read()\n",
    "        joint_resume += f\"Resume {resume}: \\n{text} \\n\\n\"\n",
    "        \n",
    "    return joint_resume\n",
    "    \n",
    "    \n",
    "def processBatches(model, total_num_resume, resume_dict, response_dict, tokens_limit, task=task, output_format=output_format, path=path, start_from_index=0, already_processed = None):\n",
    "    jj = start_from_index\n",
    "    encoding = tiktoken.encoding_for_model(GPT_4)\n",
    "\n",
    "    # Create a new directory for long resumes if it does not exist\n",
    "    if not os.path.exists(os.path.join(path, 'resume_long')):\n",
    "        os.makedirs(os.path.join(path, 'resume_long'))\n",
    "    \n",
    "    task_output_format_tokens = len(encoding.encode(task + output_format))\n",
    "    resume_tokens_limit = tokens_limit - task_output_format_tokens\n",
    "    \n",
    "    last_saved_time = time.time() \n",
    "    \n",
    "    with open('processed_resumes.txt', 'a') as processed_resumes_file, open('failed_resumes.txt', 'a') as failed_resumes_file:\n",
    "        while jj < total_num_resume:\n",
    "            joint_resume = \"\"\n",
    "            current_batch_resume_names = []  # to keep track of resume names in current batch\n",
    "            print(f\"processing resumes starting from index {jj}\")\n",
    "\n",
    "            while True:\n",
    "                resume = os.listdir(path)[jj]\n",
    "\n",
    "                # Skip the resume if it has already been processed\n",
    "                if already_processed is not None and resume in already_processed:\n",
    "                    jj += 1\n",
    "                    total_num_resume += 1\n",
    "                    print(f\"Resume {resume} has already been processed, skipping it.\")\n",
    "                    continue\n",
    "\n",
    "                file_path = os.path.join(path, resume)\n",
    "                with open(file_path, 'r', encoding='utf-8', errors=\"replace\") as f:\n",
    "                    text = f.read()\n",
    "\n",
    "                # Check if the single resume's token count is more than the token limit\n",
    "                single_resume_tokens = len(encoding.encode(f\" Resume {resume}: \\n {text} \\n\\n\"))\n",
    "                if single_resume_tokens > resume_tokens_limit:\n",
    "                    print(f\"Single resume's tokens exceeded the limit, moving {resume} to resume_long folder.\")\n",
    "                    shutil.move(file_path, os.path.join(path, 'resume_long', resume))\n",
    "                    jj += 1\n",
    "                    continue\n",
    "\n",
    "                joint_resume_temp = joint_resume + f\" Resume {resume}: \\n {text} \\n\\n\"\n",
    "                tokens = len(encoding.encode(joint_resume_temp))\n",
    "\n",
    "                if tokens > resume_tokens_limit:\n",
    "                    print(f\"Tokens limit exceeded with resume #{jj} {resume}, it will be processed in the next batch.\")\n",
    "                    break\n",
    "                else:\n",
    "                    joint_resume = joint_resume_temp\n",
    "                    current_batch_resume_names.append(resume)  # add the resume name to the current batch list\n",
    "                    jj += 1\n",
    "\n",
    "                if jj >= total_num_resume:\n",
    "                    break\n",
    "\n",
    "            job = task + output_format + joint_resume\n",
    "\n",
    "            print(f\"Total JOB tokens: {len(encoding.encode(job))}\\n\")\n",
    "            \n",
    "            try: \n",
    "                response = openai.ChatCompletion.create(\n",
    "                            model = model,\n",
    "                            messages = [\n",
    "                                {'role': \"system\", \"content\": \"You are a detail oriented resume parser who does not miss any information.\"},\n",
    "                                {\"role\": \"user\", \"content\": job},\n",
    "                            ],\n",
    "                            temperature = 0,\n",
    "                            presence_penalty = -0.8\n",
    "                            )\n",
    "            except HTTPError as e:\n",
    "                if response.status_code == 502:\n",
    "                    print(\"HTTPError 502: Bad Gateway. Trying again in 10 seconds.\")\n",
    "                    time.sleep(10)\n",
    "                else:\n",
    "                    print(\"HTTPError: \", str(e))\n",
    "                \n",
    "            except RequestException as e:\n",
    "                print(f\"There was a problem with the request: {str(e)}\")\n",
    "            \n",
    "            if response.choices[0].finish_reason != 'stop':\n",
    "                resume_tokens_limit *= 0.9\n",
    "                jj -= len(current_batch_resume_names)\n",
    "                print(\"WARNING: response did not finish. Reducing token limit by factor of 0.1\\nCurrent token limit: \", tokens_limit)\n",
    "                continue\n",
    "            \n",
    "            extracted_resume = response['choices'][0][\"message\"][\"content\"] + \"\\n\\n\"\n",
    "            response_dict[f\"{jj-len(current_batch_resume_names)}_{jj-1}\"] = response\n",
    "            try:\n",
    "                resume_dict.update(eval(extracted_resume))\n",
    "                processed_resumes_file.write(resume + '\\n')\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to parse the resume due to {str(e)}. Recording the failed resumes.\")\n",
    "                failed_resumes_file.write('\\n'.join(current_batch_resume_names) + '\\n')  # write the failed resume names\n",
    "            \n",
    "            print(f\"Finished processing resumes up to index {jj}. \\t Total tokens: {response['usage'].total_tokens}\\n\")\n",
    "            \n",
    "            current_time = time.time()\n",
    "            if current_time - last_saved_time > 8 * 60:  # 8 minutes have passed\n",
    "                with open('parsed_resume.json', 'w') as json_file:\n",
    "                    json.dump(resume_dict, json_file, indent = 2)\n",
    "                print(\"saved the resume dict as json file\")\n",
    "                last_saved_time = current_time  \n",
    "                \n",
    "        end_index = jj\n",
    "    \n",
    "    return resume_dict, response_dict, end_index\n",
    "\n",
    "    \n",
    "# Usage started at $1.72\n",
    "# Total started at $107.65\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4435"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "already_processed = [x.split('_')[1]+'.txt' if \"_\" in x else x + \".txt\" for x in list(gpt35_resume_dict.keys())]\n",
    "len(already_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.42671009771987"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing resumes starting from index 4359\n",
      "Tokens limit exceeded with resume #4364 18527.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 5183\n",
      "\n",
      "Finished processing resumes up to index 4364. \t Total tokens: 10840\n",
      "\n",
      "processing resumes starting from index 4364\n",
      "Tokens limit exceeded with resume #4366 01076.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 3433\n",
      "\n",
      "Finished processing resumes up to index 4366. \t Total tokens: 7271\n",
      "\n",
      "processing resumes starting from index 4366\n",
      "Tokens limit exceeded with resume #4369 11265.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 5751\n",
      "\n",
      "Finished processing resumes up to index 4369. \t Total tokens: 10892\n",
      "\n",
      "processing resumes starting from index 4369\n",
      "Tokens limit exceeded with resume #4371 08720.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 5493\n",
      "\n",
      "Finished processing resumes up to index 4371. \t Total tokens: 11462\n",
      "\n",
      "processing resumes starting from index 4371\n",
      "Tokens limit exceeded with resume #4373 18533.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 3503\n",
      "\n",
      "Finished processing resumes up to index 4373. \t Total tokens: 7044\n",
      "\n",
      "processing resumes starting from index 4373\n",
      "Tokens limit exceeded with resume #4375 11271.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4637\n",
      "\n",
      "Finished processing resumes up to index 4375. \t Total tokens: 9531\n",
      "\n",
      "processing resumes starting from index 4375\n",
      "Tokens limit exceeded with resume #4376 12778.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 2682\n",
      "\n",
      "Finished processing resumes up to index 4376. \t Total tokens: 4712\n",
      "\n",
      "processing resumes starting from index 4376\n",
      "Tokens limit exceeded with resume #4378 15017.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 5284\n",
      "\n",
      "Finished processing resumes up to index 4378. \t Total tokens: 10488\n",
      "\n",
      "processing resumes starting from index 4378\n",
      "Tokens limit exceeded with resume #4381 13466.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 5609\n",
      "\n",
      "Finished processing resumes up to index 4381. \t Total tokens: 11840\n",
      "\n",
      "saved the resume dict as json file\n",
      "processing resumes starting from index 4381\n",
      "Single resume's tokens exceeded the limit, moving 13300.txt to resume_long folder.\n",
      "Tokens limit exceeded with resume #4385 05562.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 3702\n",
      "\n",
      "Finished processing resumes up to index 4385. \t Total tokens: 7691\n",
      "\n",
      "processing resumes starting from index 4385\n",
      "Tokens limit exceeded with resume #4388 11517.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4543\n",
      "\n",
      "Finished processing resumes up to index 4388. \t Total tokens: 9378\n",
      "\n",
      "processing resumes starting from index 4388\n",
      "Tokens limit exceeded with resume #4390 16278.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4277\n",
      "\n",
      "Finished processing resumes up to index 4390. \t Total tokens: 9233\n",
      "\n",
      "processing resumes starting from index 4390\n",
      "Tokens limit exceeded with resume #4393 18255.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 3978\n",
      "\n",
      "Finished processing resumes up to index 4393. \t Total tokens: 7879\n",
      "\n",
      "processing resumes starting from index 4393\n",
      "Tokens limit exceeded with resume #4397 10609.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 5040\n",
      "\n",
      "Finished processing resumes up to index 4397. \t Total tokens: 10036\n",
      "\n",
      "processing resumes starting from index 4397\n",
      "Tokens limit exceeded with resume #4404 22136.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 5833\n",
      "\n",
      "Finished processing resumes up to index 4404. \t Total tokens: 13284\n",
      "\n",
      "processing resumes starting from index 4404\n",
      "Tokens limit exceeded with resume #4407 04697.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 5709\n",
      "\n",
      "WARNING: response did not finish. Reducing token limit by factor of 0.1\n",
      "Current token limit:  5851.0\n",
      "processing resumes starting from index 4404\n",
      "Tokens limit exceeded with resume #4406 25659.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4920\n",
      "\n",
      "Finished processing resumes up to index 4406. \t Total tokens: 10404\n",
      "\n",
      "saved the resume dict as json file\n",
      "processing resumes starting from index 4406\n",
      "Tokens limit exceeded with resume #4411 14484.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 3182\n",
      "\n",
      "Finished processing resumes up to index 4411. \t Total tokens: 6769\n",
      "\n",
      "processing resumes starting from index 4411\n",
      "Tokens limit exceeded with resume #4415 12793.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4213\n",
      "\n",
      "Finished processing resumes up to index 4415. \t Total tokens: 11593\n",
      "\n",
      "processing resumes starting from index 4415\n",
      "Tokens limit exceeded with resume #4416 03846.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 2525\n",
      "\n",
      "Finished processing resumes up to index 4416. \t Total tokens: 5094\n",
      "\n",
      "processing resumes starting from index 4416\n",
      "Tokens limit exceeded with resume #4419 08913.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 5164\n",
      "\n",
      "Finished processing resumes up to index 4419. \t Total tokens: 9773\n",
      "\n",
      "processing resumes starting from index 4419\n",
      "Tokens limit exceeded with resume #4426 10184.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 3901\n",
      "\n",
      "Finished processing resumes up to index 4426. \t Total tokens: 9027\n",
      "\n",
      "processing resumes starting from index 4426\n",
      "Tokens limit exceeded with resume #4431 29711.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4101\n",
      "\n",
      "Finished processing resumes up to index 4431. \t Total tokens: 8159\n",
      "\n",
      "processing resumes starting from index 4431\n",
      "Tokens limit exceeded with resume #4432 17827.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 3136\n",
      "\n",
      "Finished processing resumes up to index 4432. \t Total tokens: 6172\n",
      "\n",
      "processing resumes starting from index 4432\n",
      "Tokens limit exceeded with resume #4434 00383.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 3372\n",
      "\n",
      "Finished processing resumes up to index 4434. \t Total tokens: 6828\n",
      "\n",
      "processing resumes starting from index 4434\n",
      "Tokens limit exceeded with resume #4435 20053.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 5256\n",
      "\n",
      "Finished processing resumes up to index 4435. \t Total tokens: 11230\n",
      "\n",
      "saved the resume dict as json file\n",
      "processing resumes starting from index 4435\n",
      "Tokens limit exceeded with resume #4437 13499.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 1951\n",
      "\n",
      "Finished processing resumes up to index 4437. \t Total tokens: 4155\n",
      "\n",
      "processing resumes starting from index 4437\n",
      "Tokens limit exceeded with resume #4438 03852.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 3841\n",
      "\n",
      "Finished processing resumes up to index 4438. \t Total tokens: 7830\n",
      "\n",
      "processing resumes starting from index 4438\n",
      "Tokens limit exceeded with resume #4439 12787.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4065\n",
      "\n",
      "Finished processing resumes up to index 4439. \t Total tokens: 7844\n",
      "\n",
      "processing resumes starting from index 4439\n",
      "Tokens limit exceeded with resume #4443 15956.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4890\n",
      "\n",
      "Finished processing resumes up to index 4443. \t Total tokens: 7498\n",
      "\n",
      "processing resumes starting from index 4443\n",
      "Tokens limit exceeded with resume #4447 25895.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 5154\n",
      "\n",
      "Finished processing resumes up to index 4447. \t Total tokens: 10769\n",
      "\n",
      "processing resumes starting from index 4447\n",
      "Tokens limit exceeded with resume #4452 26344.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4433\n",
      "\n",
      "Finished processing resumes up to index 4452. \t Total tokens: 9494\n",
      "\n",
      "processing resumes starting from index 4452\n",
      "Tokens limit exceeded with resume #4455 16287.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4267\n",
      "\n",
      "Finished processing resumes up to index 4455. \t Total tokens: 6210\n",
      "\n",
      "processing resumes starting from index 4455\n",
      "Tokens limit exceeded with resume #4459 18282.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4777\n",
      "\n",
      "Finished processing resumes up to index 4459. \t Total tokens: 9545\n",
      "\n",
      "processing resumes starting from index 4459\n",
      "Tokens limit exceeded with resume #4466 29739.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 5223\n",
      "\n",
      "Finished processing resumes up to index 4466. \t Total tokens: 11862\n",
      "\n",
      "processing resumes starting from index 4466\n",
      "Tokens limit exceeded with resume #4467 28427.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 2898\n",
      "\n",
      "Finished processing resumes up to index 4467. \t Total tokens: 6013\n",
      "\n",
      "saved the resume dict as json file\n",
      "processing resumes starting from index 4467\n",
      "Tokens limit exceeded with resume #4468 27714.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4766\n",
      "\n",
      "Failed to parse the resume due to '{' was never closed (<string>, line 1). Recording the failed resumes.\n",
      "Finished processing resumes up to index 4468. \t Total tokens: 9048\n",
      "\n",
      "processing resumes starting from index 4468\n",
      "Tokens limit exceeded with resume #4473 25103.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 5189\n",
      "\n",
      "Finished processing resumes up to index 4473. \t Total tokens: 12536\n",
      "\n",
      "processing resumes starting from index 4473\n",
      "Tokens limit exceeded with resume #4475 23566.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4588\n",
      "\n",
      "Finished processing resumes up to index 4475. \t Total tokens: 8832\n",
      "\n",
      "processing resumes starting from index 4475\n",
      "Tokens limit exceeded with resume #4479 21371.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4973\n",
      "\n",
      "Finished processing resumes up to index 4479. \t Total tokens: 10582\n",
      "\n",
      "processing resumes starting from index 4479\n",
      "Tokens limit exceeded with resume #4482 27700.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4330\n",
      "\n",
      "Finished processing resumes up to index 4482. \t Total tokens: 9712\n",
      "\n",
      "processing resumes starting from index 4482\n",
      "Tokens limit exceeded with resume #4486 28355.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 3616\n",
      "\n",
      "Finished processing resumes up to index 4486. \t Total tokens: 7996\n",
      "\n",
      "processing resumes starting from index 4486\n",
      "Tokens limit exceeded with resume #4491 10812.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4842\n",
      "\n",
      "Finished processing resumes up to index 4491. \t Total tokens: 10095\n",
      "\n",
      "processing resumes starting from index 4491\n",
      "Tokens limit exceeded with resume #4495 23200.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 5213\n",
      "\n",
      "Finished processing resumes up to index 4495. \t Total tokens: 10208\n",
      "\n",
      "saved the resume dict as json file\n",
      "processing resumes starting from index 4495\n",
      "Tokens limit exceeded with resume #4504 23567.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 5052\n",
      "\n",
      "Finished processing resumes up to index 4504. \t Total tokens: 11567\n",
      "\n",
      "processing resumes starting from index 4504\n",
      "Tokens limit exceeded with resume #4507 25670.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4212\n",
      "\n",
      "Finished processing resumes up to index 4507. \t Total tokens: 8666\n",
      "\n",
      "processing resumes starting from index 4507\n",
      "Tokens limit exceeded with resume #4508 04866.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 2963\n",
      "\n",
      "Finished processing resumes up to index 4508. \t Total tokens: 5961\n",
      "\n",
      "processing resumes starting from index 4508\n",
      "Tokens limit exceeded with resume #4511 21416.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 5136\n",
      "\n",
      "Finished processing resumes up to index 4511. \t Total tokens: 9093\n",
      "\n",
      "processing resumes starting from index 4511\n",
      "Tokens limit exceeded with resume #4517 20708.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4806\n",
      "\n",
      "Finished processing resumes up to index 4517. \t Total tokens: 10484\n",
      "\n",
      "processing resumes starting from index 4517\n",
      "Tokens limit exceeded with resume #4518 10807.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4219\n",
      "\n",
      "Finished processing resumes up to index 4518. \t Total tokens: 8469\n",
      "\n",
      "processing resumes starting from index 4518\n",
      "Tokens limit exceeded with resume #4519 21402.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 1709\n",
      "\n",
      "Finished processing resumes up to index 4519. \t Total tokens: 3689\n",
      "\n",
      "processing resumes starting from index 4519\n",
      "Tokens limit exceeded with resume #4520 08090.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4921\n",
      "\n",
      "Finished processing resumes up to index 4520. \t Total tokens: 9251\n",
      "\n",
      "processing resumes starting from index 4520\n",
      "Tokens limit exceeded with resume #4524 23215.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4889\n",
      "\n",
      "Finished processing resumes up to index 4524. \t Total tokens: 10366\n",
      "\n",
      "processing resumes starting from index 4524\n",
      "Tokens limit exceeded with resume #4529 12976.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4298\n",
      "\n",
      "Finished processing resumes up to index 4529. \t Total tokens: 9141\n",
      "\n",
      "saved the resume dict as json file\n",
      "processing resumes starting from index 4529\n",
      "Tokens limit exceeded with resume #4530 06903.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4103\n",
      "\n",
      "Finished processing resumes up to index 4530. \t Total tokens: 7114\n",
      "\n",
      "processing resumes starting from index 4530\n",
      "Tokens limit exceeded with resume #4533 28426.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4931\n",
      "\n",
      "Finished processing resumes up to index 4533. \t Total tokens: 10362\n",
      "\n",
      "processing resumes starting from index 4533\n",
      "Tokens limit exceeded with resume #4534 29738.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 688\n",
      "\n",
      "Finished processing resumes up to index 4534. \t Total tokens: 1346\n",
      "\n",
      "processing resumes starting from index 4534\n",
      "Tokens limit exceeded with resume #4535 21364.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 5230\n",
      "\n",
      "Finished processing resumes up to index 4535. \t Total tokens: 9776\n",
      "\n",
      "processing resumes starting from index 4535\n",
      "Single resume's tokens exceeded the limit, moving 12786.txt to resume_long folder.\n",
      "Tokens limit exceeded with resume #4540 13498.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4389\n",
      "\n",
      "Finished processing resumes up to index 4540. \t Total tokens: 8459\n",
      "\n",
      "processing resumes starting from index 4540\n",
      "Tokens limit exceeded with resume #4543 10191.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4801\n",
      "\n",
      "Finished processing resumes up to index 4543. \t Total tokens: 8932\n",
      "\n",
      "processing resumes starting from index 4543\n",
      "Single resume's tokens exceeded the limit, moving 17826.txt to resume_long folder.\n",
      "Tokens limit exceeded with resume #4548 29076.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 5224\n",
      "\n",
      "Finished processing resumes up to index 4548. \t Total tokens: 10040\n",
      "\n",
      "processing resumes starting from index 4548\n",
      "Tokens limit exceeded with resume #4554 17198.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 3763\n",
      "\n",
      "Finished processing resumes up to index 4554. \t Total tokens: 8984\n",
      "\n",
      "processing resumes starting from index 4554\n",
      "Tokens limit exceeded with resume #4558 04682.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4969\n",
      "\n",
      "Finished processing resumes up to index 4558. \t Total tokens: 10554\n",
      "\n",
      "processing resumes starting from index 4558\n",
      "Tokens limit exceeded with resume #4563 14485.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4038\n",
      "\n",
      "Finished processing resumes up to index 4563. \t Total tokens: 9379\n",
      "\n",
      "saved the resume dict as json file\n",
      "processing resumes starting from index 4563\n",
      "Tokens limit exceeded with resume #4568 05588.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 5187\n",
      "\n",
      "Finished processing resumes up to index 4568. \t Total tokens: 10715\n",
      "\n",
      "processing resumes starting from index 4568\n",
      "Tokens limit exceeded with resume #4573 26351.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4940\n",
      "\n",
      "Finished processing resumes up to index 4573. \t Total tokens: 10397\n",
      "\n",
      "processing resumes starting from index 4573\n",
      "Tokens limit exceeded with resume #4575 20720.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 2029\n",
      "\n",
      "Finished processing resumes up to index 4575. \t Total tokens: 3851\n",
      "\n",
      "processing resumes starting from index 4575\n",
      "Tokens limit exceeded with resume #4577 00396.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 5175\n",
      "\n",
      "Finished processing resumes up to index 4577. \t Total tokens: 9883\n",
      "\n",
      "processing resumes starting from index 4577\n",
      "Tokens limit exceeded with resume #4578 27729.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 2454\n",
      "\n",
      "Finished processing resumes up to index 4578. \t Total tokens: 5367\n",
      "\n",
      "processing resumes starting from index 4578\n",
      "Tokens limit exceeded with resume #4579 10185.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4053\n",
      "\n",
      "Finished processing resumes up to index 4579. \t Total tokens: 8234\n",
      "\n",
      "processing resumes starting from index 4579\n",
      "Tokens limit exceeded with resume #4585 01088.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4584\n",
      "\n",
      "Finished processing resumes up to index 4585. \t Total tokens: 8922\n",
      "\n",
      "processing resumes starting from index 4585\n",
      "Tokens limit exceeded with resume #4589 12792.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 3264\n",
      "\n",
      "Finished processing resumes up to index 4589. \t Total tokens: 7662\n",
      "\n",
      "processing resumes starting from index 4589\n",
      "Tokens limit exceeded with resume #4593 13467.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4586\n",
      "\n",
      "Finished processing resumes up to index 4593. \t Total tokens: 9360\n",
      "\n",
      "processing resumes starting from index 4593\n",
      "Tokens limit exceeded with resume #4594 14308.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 3555\n",
      "\n",
      "Finished processing resumes up to index 4594. \t Total tokens: 6296\n",
      "\n",
      "saved the resume dict as json file\n",
      "processing resumes starting from index 4594\n",
      "Tokens limit exceeded with resume #4597 05205.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4350\n",
      "\n",
      "Finished processing resumes up to index 4597. \t Total tokens: 9002\n",
      "\n",
      "processing resumes starting from index 4597\n",
      "Tokens limit exceeded with resume #4600 01063.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 5181\n",
      "\n",
      "Finished processing resumes up to index 4600. \t Total tokens: 9744\n",
      "\n",
      "processing resumes starting from index 4600\n",
      "Tokens limit exceeded with resume #4601 18532.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4380\n",
      "\n",
      "Finished processing resumes up to index 4601. \t Total tokens: 7634\n",
      "\n",
      "processing resumes starting from index 4601\n",
      "Tokens limit exceeded with resume #4603 08721.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4718\n",
      "\n",
      "Finished processing resumes up to index 4603. \t Total tokens: 9627\n",
      "\n",
      "processing resumes starting from index 4603\n",
      "Tokens limit exceeded with resume #4609 18254.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 5113\n",
      "\n",
      "Finished processing resumes up to index 4609. \t Total tokens: 10508\n",
      "\n",
      "processing resumes starting from index 4609\n",
      "Tokens limit exceeded with resume #4611 01705.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4614\n",
      "\n",
      "Failed to parse the resume due to '{' was never closed (<string>, line 1). Recording the failed resumes.\n",
      "Finished processing resumes up to index 4611. \t Total tokens: 7243\n",
      "\n",
      "processing resumes starting from index 4611\n",
      "Tokens limit exceeded with resume #4616 20913.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4365\n",
      "\n",
      "Finished processing resumes up to index 4616. \t Total tokens: 8870\n",
      "\n",
      "processing resumes starting from index 4616\n",
      "Single resume's tokens exceeded the limit, moving 03112.txt to resume_long folder.\n",
      "Tokens limit exceeded with resume #4620 05577.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 5183\n",
      "\n",
      "Finished processing resumes up to index 4620. \t Total tokens: 10262\n",
      "\n",
      "processing resumes starting from index 4620\n",
      "Tokens limit exceeded with resume #4623 03106.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 3600\n",
      "\n",
      "Finished processing resumes up to index 4623. \t Total tokens: 7824\n",
      "\n",
      "saved the resume dict as json file\n",
      "processing resumes starting from index 4623\n",
      "Single resume's tokens exceeded the limit, moving 13315.txt to resume_long folder.\n",
      "Tokens limit exceeded with resume #4626 07360.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4018\n",
      "\n",
      "Finished processing resumes up to index 4626. \t Total tokens: 7632\n",
      "\n",
      "processing resumes starting from index 4626\n",
      "Tokens limit exceeded with resume #4628 18240.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4086\n",
      "\n",
      "Finished processing resumes up to index 4628. \t Total tokens: 8035\n",
      "\n",
      "processing resumes starting from index 4628\n",
      "Tokens limit exceeded with resume #4630 01711.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4790\n",
      "\n",
      "Finished processing resumes up to index 4630. \t Total tokens: 9532\n",
      "\n",
      "processing resumes starting from index 4630\n",
      "Tokens limit exceeded with resume #4634 06718.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4562\n",
      "\n",
      "Finished processing resumes up to index 4634. \t Total tokens: 8316\n",
      "\n",
      "processing resumes starting from index 4634\n",
      "Single resume's tokens exceeded the limit, moving 18526.txt to resume_long folder.\n",
      "Tokens limit exceeded with resume #4639 08735.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4870\n",
      "\n",
      "Finished processing resumes up to index 4639. \t Total tokens: 10269\n",
      "\n",
      "processing resumes starting from index 4639\n",
      "Tokens limit exceeded with resume #4642 13473.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 2625\n",
      "\n",
      "Finished processing resumes up to index 4642. \t Total tokens: 5302\n",
      "\n",
      "processing resumes starting from index 4642\n",
      "Tokens limit exceeded with resume #4644 15002.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 5156\n",
      "\n",
      "Finished processing resumes up to index 4644. \t Total tokens: 14909\n",
      "\n",
      "processing resumes starting from index 4644\n",
      "Tokens limit exceeded with resume #4648 16523.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 5098\n",
      "\n",
      "Finished processing resumes up to index 4648. \t Total tokens: 10117\n",
      "\n",
      "processing resumes starting from index 4648\n",
      "Tokens limit exceeded with resume #4653 10152.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 5013\n",
      "\n",
      "Finished processing resumes up to index 4653. \t Total tokens: 11211\n",
      "\n",
      "saved the resume dict as json file\n",
      "processing resumes starting from index 4653\n",
      "Tokens limit exceeded with resume #4655 03648.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4931\n",
      "\n",
      "Finished processing resumes up to index 4655. \t Total tokens: 8909\n",
      "\n",
      "processing resumes starting from index 4655\n",
      "Tokens limit exceeded with resume #4659 22686.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 5269\n",
      "\n",
      "Finished processing resumes up to index 4659. \t Total tokens: 9651\n",
      "\n",
      "processing resumes starting from index 4659\n",
      "Tokens limit exceeded with resume #4662 05239.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4877\n",
      "\n",
      "Finished processing resumes up to index 4662. \t Total tokens: 6742\n",
      "\n",
      "processing resumes starting from index 4662\n",
      "Tokens limit exceeded with resume #4667 14452.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 2761\n",
      "\n",
      "Finished processing resumes up to index 4667. \t Total tokens: 5413\n",
      "\n",
      "processing resumes starting from index 4667\n",
      "Tokens limit exceeded with resume #4670 15994.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4533\n",
      "\n",
      "Finished processing resumes up to index 4670. \t Total tokens: 8140\n",
      "\n",
      "processing resumes starting from index 4670\n",
      "Tokens limit exceeded with resume #4671 07348.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4693\n",
      "\n",
      "Finished processing resumes up to index 4671. \t Total tokens: 8977\n",
      "\n",
      "processing resumes starting from index 4671\n",
      "Tokens limit exceeded with resume #4674 00427.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4125\n",
      "\n",
      "Finished processing resumes up to index 4674. \t Total tokens: 8779\n",
      "\n",
      "processing resumes starting from index 4674\n",
      "Tokens limit exceeded with resume #4677 19176.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 3401\n",
      "\n",
      "Finished processing resumes up to index 4677. \t Total tokens: 6791\n",
      "\n",
      "processing resumes starting from index 4677\n",
      "Tokens limit exceeded with resume #4678 01739.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 3058\n",
      "\n",
      "Finished processing resumes up to index 4678. \t Total tokens: 5836\n",
      "\n",
      "processing resumes starting from index 4678\n",
      "Tokens limit exceeded with resume #4682 10620.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 5292\n",
      "\n",
      "Finished processing resumes up to index 4682. \t Total tokens: 11809\n",
      "\n",
      "processing resumes starting from index 4682\n",
      "Tokens limit exceeded with resume #4685 19162.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 3666\n",
      "\n",
      "Finished processing resumes up to index 4685. \t Total tokens: 8151\n",
      "\n",
      "saved the resume dict as json file\n",
      "processing resumes starting from index 4685\n",
      "Tokens limit exceeded with resume #4689 12037.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4620\n",
      "\n",
      "Finished processing resumes up to index 4689. \t Total tokens: 9436\n",
      "\n",
      "processing resumes starting from index 4689\n",
      "Tokens limit exceeded with resume #4692 25843.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 2974\n",
      "\n",
      "Finished processing resumes up to index 4692. \t Total tokens: 6090\n",
      "\n",
      "processing resumes starting from index 4692\n",
      "Tokens limit exceeded with resume #4695 13329.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4099\n",
      "\n",
      "Finished processing resumes up to index 4695. \t Total tokens: 5656\n",
      "\n",
      "processing resumes starting from index 4695\n",
      "Single resume's tokens exceeded the limit, moving 15980.txt to resume_long folder.\n",
      "Tokens limit exceeded with resume #4698 12989.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4331\n",
      "\n",
      "Finished processing resumes up to index 4698. \t Total tokens: 8139\n",
      "\n",
      "processing resumes starting from index 4698\n",
      "Tokens limit exceeded with resume #4701 22692.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4184\n",
      "\n",
      "Finished processing resumes up to index 4701. \t Total tokens: 8045\n",
      "\n",
      "processing resumes starting from index 4701\n",
      "Tokens limit exceeded with resume #4703 12751.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 2593\n",
      "\n",
      "Finished processing resumes up to index 4703. \t Total tokens: 5680\n",
      "\n",
      "processing resumes starting from index 4703\n",
      "Tokens limit exceeded with resume #4704 06724.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4982\n",
      "\n",
      "Finished processing resumes up to index 4704. \t Total tokens: 9311\n",
      "\n",
      "processing resumes starting from index 4704\n",
      "Tokens limit exceeded with resume #4708 27932.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4954\n",
      "\n",
      "Finished processing resumes up to index 4708. \t Total tokens: 11198\n",
      "\n",
      "processing resumes starting from index 4708\n",
      "Tokens limit exceeded with resume #4711 17629.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4502\n",
      "\n",
      "Finished processing resumes up to index 4711. \t Total tokens: 8084\n",
      "\n",
      "processing resumes starting from index 4711\n",
      "Tokens limit exceeded with resume #4714 08709.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4953\n",
      "\n",
      "Finished processing resumes up to index 4714. \t Total tokens: 10399\n",
      "\n",
      "processing resumes starting from index 4714\n",
      "Tokens limit exceeded with resume #4716 28156.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4382\n",
      "\n",
      "Finished processing resumes up to index 4716. \t Total tokens: 8721\n",
      "\n",
      "saved the resume dict as json file\n",
      "processing resumes starting from index 4716\n",
      "Tokens limit exceeded with resume #4722 21614.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4421\n",
      "\n",
      "Finished processing resumes up to index 4722. \t Total tokens: 8096\n",
      "\n",
      "processing resumes starting from index 4722\n",
      "Tokens limit exceeded with resume #4727 02973.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4679\n",
      "\n",
      "Finished processing resumes up to index 4727. \t Total tokens: 9943\n",
      "\n",
      "processing resumes starting from index 4727\n",
      "Tokens limit exceeded with resume #4729 21172.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 2307\n",
      "\n",
      "Finished processing resumes up to index 4729. \t Total tokens: 4725\n",
      "\n",
      "processing resumes starting from index 4729\n",
      "Tokens limit exceeded with resume #4732 27503.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 5161\n",
      "\n",
      "Finished processing resumes up to index 4732. \t Total tokens: 10041\n",
      "\n",
      "processing resumes starting from index 4732\n",
      "Tokens limit exceeded with resume #4738 27517.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4929\n",
      "\n",
      "Finished processing resumes up to index 4738. \t Total tokens: 11037\n",
      "\n",
      "processing resumes starting from index 4738\n",
      "Tokens limit exceeded with resume #4742 02967.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 3641\n",
      "\n",
      "Finished processing resumes up to index 4742. \t Total tokens: 8349\n",
      "\n",
      "processing resumes starting from index 4742\n",
      "Tokens limit exceeded with resume #4747 24778.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 5113\n",
      "\n",
      "Finished processing resumes up to index 4747. \t Total tokens: 8404\n",
      "\n",
      "processing resumes starting from index 4747\n",
      "Tokens limit exceeded with resume #4749 18081.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4228\n",
      "\n",
      "Finished processing resumes up to index 4749. \t Total tokens: 8021\n",
      "\n",
      "processing resumes starting from index 4749\n",
      "Tokens limit exceeded with resume #4752 27271.txt, it will be processed in the next batch.\n",
      "Total JOB tokens: 4518\n",
      "\n"
     ]
    },
    {
     "ename": "ServiceUnavailableError",
     "evalue": "The server is overloaded or not ready yet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mServiceUnavailableError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[199], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m task \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mturn the following resumes into JSON format with their file names as their keys not including the file extension. Parse the ENTIRE resume, and do not omit any part due to length! Do not add any extra sections!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m already_processed \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m1\u001b[39m]\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.txt\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m x \u001b[39melse\u001b[39;00m x \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.txt\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(gpt35_resume_dict\u001b[39m.\u001b[39mkeys())]\n\u001b[0;32m----> 5\u001b[0m processBatches(model \u001b[39m=\u001b[39;49m GPT_35_16K, \n\u001b[1;32m      6\u001b[0m                total_num_resume \u001b[39m=\u001b[39;49m \u001b[39m4802\u001b[39;49m, \n\u001b[1;32m      7\u001b[0m                resume_dict \u001b[39m=\u001b[39;49m gpt35_resume_dict, \n\u001b[1;32m      8\u001b[0m                response_dict \u001b[39m=\u001b[39;49m gpt35_response_dict, \n\u001b[1;32m      9\u001b[0m                tokens_limit \u001b[39m=\u001b[39;49m \u001b[39m16384\u001b[39;49m \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m \u001b[39m2.8\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m                output_format \u001b[39m=\u001b[39;49m output_format_skeleton,\n\u001b[1;32m     11\u001b[0m                start_from_index \u001b[39m=\u001b[39;49m \u001b[39m4359\u001b[39;49m,\n\u001b[1;32m     12\u001b[0m                already_processed \u001b[39m=\u001b[39;49m already_processed);\n",
      "Cell \u001b[0;32mIn[171], line 74\u001b[0m, in \u001b[0;36mprocessBatches\u001b[0;34m(model, total_num_resume, resume_dict, response_dict, tokens_limit, task, output_format, path, start_from_index, already_processed)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTotal JOB tokens: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(encoding\u001b[39m.\u001b[39mencode(job))\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     73\u001b[0m \u001b[39mtry\u001b[39;00m: \n\u001b[0;32m---> 74\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     75\u001b[0m                 model \u001b[39m=\u001b[39;49m model,\n\u001b[1;32m     76\u001b[0m                 messages \u001b[39m=\u001b[39;49m [\n\u001b[1;32m     77\u001b[0m                     {\u001b[39m'\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mYou are a detail oriented resume parser who does not miss any information.\u001b[39;49m\u001b[39m\"\u001b[39;49m},\n\u001b[1;32m     78\u001b[0m                     {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: job},\n\u001b[1;32m     79\u001b[0m                 ],\n\u001b[1;32m     80\u001b[0m                 temperature \u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m,\n\u001b[1;32m     81\u001b[0m                 presence_penalty \u001b[39m=\u001b[39;49m \u001b[39m-\u001b[39;49m\u001b[39m0.8\u001b[39;49m\n\u001b[1;32m     82\u001b[0m                 )\n\u001b[1;32m     83\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     84\u001b[0m     \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m502\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/resume_research/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/resume_research/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/resume_research/lib/python3.10/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m/opt/anaconda3/envs/resume_research/lib/python3.10/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    705\u001b[0m         ),\n\u001b[1;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/resume_research/lib/python3.10/site-packages/openai/api_requestor.py:743\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    740\u001b[0m     \u001b[39mreturn\u001b[39;00m OpenAIResponse(\u001b[39mNone\u001b[39;00m, rheaders)\n\u001b[1;32m    742\u001b[0m \u001b[39mif\u001b[39;00m rcode \u001b[39m==\u001b[39m \u001b[39m503\u001b[39m:\n\u001b[0;32m--> 743\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mServiceUnavailableError(\n\u001b[1;32m    744\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe server is overloaded or not ready yet.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    745\u001b[0m         rbody,\n\u001b[1;32m    746\u001b[0m         rcode,\n\u001b[1;32m    747\u001b[0m         headers\u001b[39m=\u001b[39mrheaders,\n\u001b[1;32m    748\u001b[0m     )\n\u001b[1;32m    749\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    750\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mtext/plain\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m rheaders\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mContent-Type\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m):\n",
      "\u001b[0;31mServiceUnavailableError\u001b[0m: The server is overloaded or not ready yet."
     ]
    }
   ],
   "source": [
    "path = \"resume_corpus\"\n",
    "task = \"turn the following resumes into JSON format with their file names as their keys not including the file extension. Parse the ENTIRE resume, and do not omit any part due to length! Do not add any extra sections!\"\n",
    "already_processed = [x.split('_')[1]+'.txt' if \"_\" in x else x + \".txt\" for x in list(gpt35_resume_dict.keys())]\n",
    "\n",
    "processBatches(model = GPT_35_16K, \n",
    "               total_num_resume = 4802, \n",
    "               resume_dict = gpt35_resume_dict, \n",
    "               response_dict = gpt35_response_dict, \n",
    "               tokens_limit = 16384 // 2.8,\n",
    "               output_format = output_format_skeleton,\n",
    "               start_from_index = 4359,\n",
    "               already_processed = already_processed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"parsed_resume.json\", \"w\") as f:\n",
    "    json.dump(gpt35_resume_dict, f, indent = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"parsed_resume_backup.json\", \"w\") as f:\n",
    "    json.dump(gpt35_resume_dict, f, indent = 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2687\n"
     ]
    }
   ],
   "source": [
    "with open(\"parsed_resume.json\", \"r\") as f:\n",
    "    print(len(json.load(f)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resume_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
